# Llama3 LoRA Fine-tuning for MSRA C Standard Dataset

Bu proje, Llama3 modelini MSRA C standardÄ± dataset'i ile LoRA (Low-Rank Adaptation) kullanarak fine-tuning yapmak iÃ§in tasarlanmÄ±ÅŸtÄ±r.

## ğŸš€ Ã–zellikler

- **LoRA Fine-tuning**: Bellek verimli fine-tuning iÃ§in LoRA kullanÄ±mÄ±
- **4-bit Quantization**: BitsAndBytes ile bellek optimizasyonu
- **MSRA Dataset**: C programlama dili standartlarÄ± iÃ§in Ã¶zel dataset
- **Instruction Following**: Instruction-following formatÄ±nda eÄŸitim
- **Wandb Integration**: EÄŸitim sÃ¼recini takip etmek iÃ§in

## ğŸ“‹ Gereksinimler

- Python 3.8+
- CUDA uyumlu GPU (en az 8GB VRAM Ã¶nerilir)
- Hugging Face hesabÄ± ve Llama3 model eriÅŸimi

## ğŸ› ï¸ Kurulum

1. **Repository'yi klonlayÄ±n:**
```bash
git clone <repository-url>
cd dataset
```

2. **Gerekli paketleri yÃ¼kleyin:**
```bash
pip install -r requirements.txt
```

3. **Hugging Face token'Ä±nÄ±zÄ± ayarlayÄ±n:**
```bash
huggingface-cli login
```

## ğŸ“Š Dataset

MSRA C standardÄ± dataset'i ÅŸu formatta yapÄ±landÄ±rÄ±lmÄ±ÅŸtÄ±r:
- **Instruction**: C kodu analizi iÃ§in talimat
- **Input**: Analiz edilecek C kodu
- **Output**: MSRA standardÄ±na uygunluk deÄŸerlendirmesi

## ğŸ¯ KullanÄ±m

### 1. EÄŸitim

```bash
python train_lora.py
```

**Ã–nemli Notlar:**
- `config.py` dosyasÄ±nda `BASE_MODEL_NAME`'i kendi Llama3 modelinizle deÄŸiÅŸtirin
- GPU bellek yetersizse batch size'Ä± dÃ¼ÅŸÃ¼rÃ¼n
- EÄŸitim sÃ¼resi dataset boyutuna ve hardware'e baÄŸlÄ± olarak deÄŸiÅŸir

### 2. Inference

```bash
python inference.py
```

Bu script eÄŸitilmiÅŸ modeli yÃ¼kler ve test eder.

## âš™ï¸ KonfigÃ¼rasyon

`config.py` dosyasÄ±nda aÅŸaÄŸÄ±daki parametreleri ayarlayabilirsiniz:

- **LoRA Parameters**: Rank, alpha, dropout
- **Training Parameters**: Epochs, batch size, learning rate
- **Hardware Settings**: Quantization, device mapping

## ğŸ“ Dosya YapÄ±sÄ±

```
dataset/
â”œâ”€â”€ dataset_msra_instruct.jsonl    # MSRA dataset
â”œâ”€â”€ train_lora.py                  # Ana eÄŸitim script'i
â”œâ”€â”€ inference.py                   # Inference script'i
â”œâ”€â”€ config.py                      # KonfigÃ¼rasyon dosyasÄ±
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                     # Bu dosya
```

## ğŸ”§ LoRA KonfigÃ¼rasyonu

```python
lora_config = LoraConfig(
    r=16,                          # Rank
    lora_alpha=32,                 # Alpha scaling
    target_modules=[               # Hedef modÃ¼ller
        "q_proj", "v_proj", "k_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)
```

## ğŸ“ˆ EÄŸitim Metrikleri

EÄŸitim sÄ±rasÄ±nda aÅŸaÄŸÄ±daki metrikler takip edilir:
- Training loss
- Evaluation loss
- Learning rate
- GPU memory usage

## ğŸš¨ Sorun Giderme

### YaygÄ±n Hatalar:

1. **CUDA Out of Memory**: Batch size'Ä± dÃ¼ÅŸÃ¼rÃ¼n veya gradient accumulation steps'i artÄ±rÄ±n
2. **Model Loading Error**: Hugging Face token'Ä±nÄ±zÄ± kontrol edin
3. **Dataset Format Error**: JSONL formatÄ±nÄ± kontrol edin

### Bellek Optimizasyonu:

- 4-bit quantization kullanÄ±n
- Gradient checkpointing aktif edin
- Batch size'Ä± optimize edin

## ğŸ“ Ã–rnek KullanÄ±m

```python
from config import TrainingConfig

# LoRA konfigÃ¼rasyonu
lora_config = TrainingConfig.get_lora_config()

# EÄŸitim parametreleri
training_args = TrainingConfig.get_training_args()
```

## ğŸ¤ KatkÄ±da Bulunma

1. Fork yapÄ±n
2. Feature branch oluÅŸturun (`git checkout -b feature/amazing-feature`)
3. Commit yapÄ±n (`git commit -m 'Add amazing feature'`)
4. Push yapÄ±n (`git push origin feature/amazing-feature`)
5. Pull Request oluÅŸturun

## ğŸ“„ Lisans

Bu proje MIT lisansÄ± altÄ±nda lisanslanmÄ±ÅŸtÄ±r.

## ğŸ™ TeÅŸekkÃ¼rler

- Hugging Face ekibine
- PEFT (Parameter-Efficient Fine-Tuning) geliÅŸtiricilerine
- MSRA dataset'i hazÄ±rlayanlara

## ğŸ“ Ä°letiÅŸim

SorularÄ±nÄ±z iÃ§in issue aÃ§abilir veya pull request gÃ¶nderebilirsiniz.

